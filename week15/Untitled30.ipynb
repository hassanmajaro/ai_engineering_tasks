{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh2N04CVtCe8"
      },
      "source": [
        "# 1. Introduction to the Perceptron\n",
        "\n",
        "Welcome! This notebook will guide you through building your very first deep learning algorithm, the **Perceptron**, from scratch.\n",
        "\n",
        "Perceptrons were one of the first algorithms discovered in the field of AI. Their significance was that they raised the hopes and expectations for the field of neural networks. A Perceptron is a machine learning algorithm that uses a single node (or \"neuron\") to predict a class label for a row of data. It's the simplest possible type of neural network.\n",
        "\n",
        "\n",
        "\n",
        "### The Four Components of a Perceptron\n",
        "\n",
        "A Perceptron has four main parts. Think of it as a simple decision-making process:\n",
        "\n",
        "1.  **Input Values ($x$):** This is your data. For example, in our case, we'll have two input values, `x1` and `x2`.\n",
        "2.  **Weights ($w$) and Bias ($b$):**\n",
        "    * **Weights:** Each input value has a corresponding \"weight\". A weight represents how important that input is for the final decision. A bigger weight means that input matters more.\n",
        "    * **Bias:** The bias is an extra value that helps the model make a decision.\n",
        "    * *Learning* is simply the process of finding the perfect set of weights and bias.\n",
        "3.  **Net Sum ($z$):** This is the first calculation the Perceptron does. It multiplies each input by its weight, sums them all up, and then adds the bias.\n",
        "\n",
        "    $z = (x_1 \\cdot w_1) + (x_2 \\cdot w_2) + ... + b$\n",
        "\n",
        "4.  **Activation Function:** This is the final step. The Perceptron takes the `Net Sum` ($z$) and passes it through a simple function to make a final, binary decision (0 or 1).\n",
        "    * The simplest activation function is a **step function**:\n",
        "    * If the `Net Sum` ($z$) is greater than 0, predict **1**.\n",
        "    * Otherwise, predict **0**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdBRsmmmtMWd"
      },
      "source": [
        "# 2. Setup and Creating the Dataset\n",
        "\n",
        "First, let's import the libraries we'll need.\n",
        "\n",
        "* `numpy`: For numerical operations.\n",
        "* `pandas`: To easily look at our data in a table.\n",
        "* `matplotlib`: For plotting our data.\n",
        "* `sklearn.datasets.make_classification`: A handy function to create a simple, fake dataset for us to play with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUmiUxj1tAsm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNmhmP-btRa_"
      },
      "source": [
        "### Creating the dataset\n",
        "\n",
        "Here we generate a simulated dataset using scikit-learn's `make_classification` function. This creates a dataset with 2 input features and binary target labels.\n",
        "\n",
        "We're asking for:\n",
        "* `n_samples = 20`: 20 rows of data.\n",
        "* `n_features = 2`: 2 input features (we'll call them `x1` and `x2`).\n",
        "* `n_informative = 1`: Only 1 of the features is *actually* useful for telling the classes apart.\n",
        "* `n_redundant = 0`: No extra, useless features.\n",
        "* `n_clusters_per_class = 1`: Each class (0 and 1) is just one single \"blob\" of data.\n",
        "* `random_state = 1`: This ensures that if you run this code, you get the *exact same* \"random\" data that I do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cwftGf2tR7C"
      },
      "outputs": [],
      "source": [
        "features, targets = make_classification(n_samples = 20,\n",
        "                                      n_features = 2,\n",
        "                                      n_informative = 1,\n",
        "                                      n_redundant = 0,\n",
        "                                      n_clusters_per_class = 1,\n",
        "                                      random_state = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAGyDDwptU2Z"
      },
      "source": [
        "Let's put this data into a `pandas` DataFrame to see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X74O6t5TtaJU"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data=features, columns=['x1', 'x2'])\n",
        "df['targets'] = targets\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEZ-mNvTtYY_"
      },
      "source": [
        "Let's check the `shape` of our data. This tells us (rows, columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBPtz6c-teGk"
      },
      "outputs": [],
      "source": [
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvH4ds5ktfyr"
      },
      "outputs": [],
      "source": [
        "targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exR8bhWWthq2"
      },
      "source": [
        "`np.bincount` is a quick way to count how many samples of each class we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98T4i1WctjfA"
      },
      "outputs": [],
      "source": [
        "np.bincount(targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3lqOsM6tlqC"
      },
      "source": [
        "This confirms we have 10 samples of Class 0 and 10 samples of Class 1. A nice, balanced dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbWQKbxBtoMk"
      },
      "source": [
        "# 3. Visualizing the dataset\n",
        "\n",
        "We can visualize the dataset by plotting the two input features colored by the target class. This gives us a sense of how linearly separable the data is.\n",
        "\n",
        "A **linearly separable** dataset is one where you can draw a single straight line to separate the two classes. A Perceptron can *only* solve problems that are linearly separable.\n",
        "\n",
        "As we can see below, our data isn't *perfectly* separable, but there is an approximate linear decision boundary. Let's see if our Perceptron can find it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzFPltyVtrP7"
      },
      "outputs": [],
      "source": [
        "plt.plot(\n",
        "    features[targets == 0, 0],\n",
        "    features[targets == 0, 1],\n",
        "    marker = 'P',\n",
        "    markersize = 10,\n",
        "    linestyle = '',\n",
        "    label = 'Class 0'\n",
        "    )\n",
        "plt.plot(\n",
        "    features[targets == 1, 0],\n",
        "    features[targets == 1, 1],\n",
        "    marker = '^',\n",
        "    markersize = 10,\n",
        "    linestyle = '',\n",
        "    label = 'Class 1')\n",
        "plt.legend(loc = 2)\n",
        "plt.xlim(-2, 2)\n",
        "plt.ylim(-2, 2)\n",
        "plt.xlabel(\"Feature $x_1$\", fontsize=12)\n",
        "plt.ylabel(\"Feature $x_2$\", fontsize=12)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUcY-cAwtuGW"
      },
      "source": [
        "# 4. Implementing a Perceptron\n",
        "\n",
        "We can now implement the perceptron algorithm in Python. We will do this by creating a `Perceptron` class.\n",
        "\n",
        "First, let's define the `__init__` method. This method is called whenever we create a new Perceptron object. It's used to set up our initial variables: `weights` and `bias`.\n",
        "\n",
        "We'll initialize all weights to `0.0` and the bias to `0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_jOw9pitwtp"
      },
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, num_features):\n",
        "        self.num_features = num_features\n",
        "        self.weights = [0.0 for _ in range(num_features)]\n",
        "        self.bias = 0\n",
        "\n",
        "ppn = Perceptron(num_features = 2)\n",
        "ppn.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9mx-mWkty5i"
      },
      "outputs": [],
      "source": [
        "ppn.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OCukvVtt1nY"
      },
      "source": [
        "### Implementing the forward function\n",
        "\n",
        "The **forward pass** is how the Perceptron makes a prediction.\n",
        "\n",
        "The `forward` function will compute the weighted sum ($z$) and then use the activation function (the step function) to return a binary prediction (0 or 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ned8HE53t1OK"
      },
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, num_features):\n",
        "        self.num_features = num_features\n",
        "        self.weights = [0.0 for _ in range(num_features)]\n",
        "        self.bias = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        weighted_sum_z = self.bias\n",
        "        for i, _ in enumerate(self.weights):\n",
        "            weighted_sum_z += x[i] * self.weights[i]\n",
        "\n",
        "        if weighted_sum_z > 0:\n",
        "            prediction = 1\n",
        "        else:\n",
        "            prediction = 0\n",
        "\n",
        "        return prediction\n",
        "\n",
        "ppn = Perceptron(num_features = 2) # Re-initialize with 2 features\n",
        "x = [1.23, 2.13]\n",
        "ppn.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfUkFntft6G4"
      },
      "source": [
        "The prediction in the above case is 0 because the weights and bias of the perceptron are still 0. It has not learned anything yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpOpfPLQt9mI"
      },
      "source": [
        "### Updating the weights (The Learning Rule)\n",
        "\n",
        "This is the most important part! The `update` method is where the *learning* happens. It takes a single training example (`x`, `y_true`), compares the `prediction` to the `y_true` label, and updates the weights and bias accordingly.\n",
        "\n",
        "The update rule is:\n",
        "1.  Calculate the `error = y_true - prediction`.\n",
        "    * If the prediction is correct (0 vs 0 or 1 vs 1), the `error` will be **0**.\n",
        "    * If it predicts 0 but should be 1, the `error` will be **1**.\n",
        "    * If it predicts 1 but should be 0, the `error` will be **-1**.\n",
        "2.  Update the bias: `self.bias += error`\n",
        "3.  Update the weights: `self.weights[i] += error * x[i]`\n",
        "\n",
        "This means that if the error is 0, **no updates happen**. The model only learns when it makes a mistake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5-WahKKt8x1"
      },
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, num_features):\n",
        "        self.num_features = num_features\n",
        "        self.weights = [0.0 for _ in range(num_features)]\n",
        "        self.bias = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        weighted_sum_z = self.bias\n",
        "        for i, _ in enumerate(self.weights):\n",
        "            weighted_sum_z += x[i] * self.weights[i]\n",
        "\n",
        "        if weighted_sum_z > 0:\n",
        "            prediction = 1\n",
        "        else:\n",
        "            prediction = 0\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def update(self, x, y_true):\n",
        "        prediction = self.forward(x)\n",
        "        error = y_true - prediction\n",
        "\n",
        "        self.bias += error\n",
        "        for i, _ in enumerate(self.weights):\n",
        "            self.weights[i] += error * x[i]\n",
        "\n",
        "        return error\n",
        "\n",
        "ppn = Perceptron(num_features = 2)\n",
        "x = [1.1, 2.1]\n",
        "ppn.update(x, y_true = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WABZntckuDXz"
      },
      "source": [
        "Let's check what happened to the weights and bias. Since the prediction was 0 (all weights were 0), and the true label was 1, the `error` was `1 - 0 = 1`.\n",
        "\n",
        "The new bias should be `0 + 1 = 1`.\n",
        "The new weights should be `[0 + (1 * 1.1), 0 + (1 * 2.1)]` = `[1.1, 2.1]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h7NROsJuF-B"
      },
      "outputs": [],
      "source": [
        "print(\"Model parameters:\")\n",
        "print(\"Weights:\", ppn.weights)\n",
        "print(\"Bias:\", ppn.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCfJiv92uH_r"
      },
      "source": [
        "# 5. Training the model\n",
        "\n",
        "Now we can write a `train` function to automate the training process.\n",
        "\n",
        "An **epoch** is one full pass through the entire training dataset. We will train the perceptron by iterating through the dataset for several epochs. In each epoch, we loop through all the `(x, y)` examples and call the `update` method for each one.\n",
        "\n",
        "We'll also count the number of errors in each epoch. As the model learns, we should see the number of errors go down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn1CZ0V6uLUc"
      },
      "outputs": [],
      "source": [
        "def train(model, X_train, y_train, epochs):\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        error_count = 0\n",
        "\n",
        "        for x, y in zip(X_train, y_train):\n",
        "            error = model.update(x, y)\n",
        "            error_count += abs(error)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} errors {error_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OaziS6buNk2"
      },
      "source": [
        "Let's create a new Perceptron and train it for 5 epochs on our `features` and `targets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygVlAXBBuP2j"
      },
      "outputs": [],
      "source": [
        "ppn = Perceptron(num_features = 2)\n",
        "train(ppn, features, targets, epochs = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD8mf17TuScz"
      },
      "source": [
        "Look at that! After just 2 epochs, the model made **0 errors**. This means it has found a set of weights and bias that perfectly separates the training data. The Perceptron algorithm is guaranteed to find a solution *if* one exists (i.e., if the data is linearly separable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf7KUMZiuVKm"
      },
      "source": [
        "# 6. Evaluating the model\n",
        "\n",
        "We can evaluate the model on the training data by comparing its predictions to the true labels and computing the prediction accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBmK-uf0uS_c"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(model, features, targets):\n",
        "    correct = 0.0\n",
        "\n",
        "    for x, y in zip(features, targets):\n",
        "        prediction = model.forward(x)\n",
        "        correct += int(prediction == y)\n",
        "\n",
        "    return correct / len(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDIK1ydcucRP"
      },
      "outputs": [],
      "source": [
        "train_acc = compute_accuracy(ppn, features, targets)\n",
        "print(\"Model Accuracy:\", train_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVUF-wX3ue5_"
      },
      "source": [
        "An accuracy of 1.0 means 100% correct. Perfect!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVbopOcXuhdW"
      },
      "source": [
        "# 7. Plotting the Decision Boundary\n",
        "\n",
        "Finally, we can visualize the perceptron's linear decision boundary. The line that the Perceptron learns is where the `weighted_sum_z` is exactly 0.\n",
        "\n",
        "$z = (w_1 \\cdot x_1) + (w_2 \\cdot x_2) + b = 0$\n",
        "\n",
        "We can solve this for $x_2$ to get the equation of a line:\n",
        "\n",
        "$x_2 = \\frac{-(w_1 \\cdot x_1) - b}{w_2}$\n",
        "\n",
        "We can use this equation to plot the line (or \"dividing hyperplane\") that our model has learned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr7IU_XIukm_"
      },
      "outputs": [],
      "source": [
        "def plot_boundary(model):\n",
        "\n",
        "    w1, w2 = model.weights[0], model.weights[1]\n",
        "    b = model.bias\n",
        "\n",
        "    # z = w1*x1 + w2*x2 + b = 0\n",
        "    # => x2 = (-w1*x1 - b) / w2\n",
        "\n",
        "    x1_min = -20\n",
        "    x2_min = (-(w1 * x1_min) - b) / w2\n",
        "\n",
        "    x1_max = 20\n",
        "    x2_max = (-(w1 * x1_max) - b) / w2\n",
        "\n",
        "    return x1_min, x1_max, x2_min, x2_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUNqXq7Tunv4"
      },
      "outputs": [],
      "source": [
        "x1_min, x1_max, x2_min, x2_max = plot_boundary(ppn)\n",
        "\n",
        "plt.plot(\n",
        "    features[targets == 0, 0],\n",
        "    features[targets == 0, 1],\n",
        "    marker=\"D\",\n",
        "    markersize=10,\n",
        "    linestyle=\"\",\n",
        "    label=\"Class 0\",\n",
        ")\n",
        "plt.plot(\n",
        "    features[targets == 1, 0],\n",
        "    features[targets == 1, 1],\n",
        "    marker=\"^\",\n",
        "    markersize=13,\n",
        "    linestyle=\"\",\n",
        "    label=\"Class 1\",\n",
        ")\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.plot([x1_min, x1_max], [x2_min, x2_max], color=\"k\")\n",
        "\n",
        "plt.legend(loc=2)\n",
        "plt.xlim([-2, 2])\n",
        "plt.ylim([-2, 2])\n",
        "plt.xlabel(\"Feature $x_1$\", fontsize=12)\n",
        "plt.ylabel(\"Feature $x_2$\", fontsize=12)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG2-m8pmup82"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "And there it is! The black line is the decision boundary learned by our Perceptron. Everything on one side of the line will be predicted as Class 0, and everything on the other side will be predicted as Class 1.\n",
        "\n",
        "This demonstrates how a simple Perceptron model can learn a linear classifier from data. While limited to linearly separable problems, the Perceptron formed the foundation for all the more complex neural networks that are used today."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "new_module",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
